---
title: "巨量資料分析"
author: "410473074 王姿文、410473002 林易霆、410473014 康益豪、410478016 張維翰"
date: "2019/06/18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message =  FALSE)
```

```{r library, include=FALSE}
library(ggplot2)
library(knitr);library(kableExtra);library(dplyr)#kable
library(mice);library(lattice); library(purrr);library(VIM)#missing data
library(dplyr);library(car);library(MASS);library(klaR)#LDA
library(rattle); library(randomForest)
library(caret)
library(LogicReg);library(doParallel)
library(corrplot) #for correlation plot
library(gridExtra)
library(tidyverse); library(e1071);
library(ROCR);library(pROC); library(stepPlr);
library(readr); library(tidyverse); library(gam);
library(kableExtra); library(neuralnet); library(nnet);
```


```{r, include = FALSE}
missing <- readRDS("RDS files/missing.rds")
soil.int <- readRDS("RDS files/data.rds")
error_rate_k <- readRDS("RDS files/k_plot.rds")
lda <- readRDS("RDS files/lda.rds")
training <- readRDS("RDS files/ldatr.rds")
tmp <- readRDS("RDS files/tmp.rds")
tree_model <- readRDS("RDS files/tree_model.rds")
randomforest_model <- readRDS("RDS files/randomforest_model.rds")
soilcat <- readRDS("RDS files/soilcat.rds")
soilcon <- readRDS("RDS files/soilcon.rds")
soilcon2 <- readRDS("RDS files/soilcon2.rds")
model1 <- readRDS("RDS files/model1.rds")
model2 <- readRDS("RDS files/model2.rds")
model3 <- readRDS("RDS files/model3.rds")
model4 <- readRDS("RDS files/model4.rds")
soil_test <- readRDS("RDS files/soil_test.rds")
soil_agg <- readRDS("RDS files/soil_agg.rds")
```

## 1. 資料說明

### 1.1 資料名稱

Soil erosion and organic matter for central Great Plains cropping systems under residue removal 

-------------------------------------------------
 
### 1.2 資料來源：

https://catalog.data.gov/dataset/soil-erosion-and-organic-matter-for-central-great-plains-cropping-systems-under-residue-re?fbclid=IwAR3cLtZiyGGUOoCgmnmMnCkQkPiQe2yGKGW_1qXyfgDWGbI5kO3E6Ivpkyg

-------------------------------------------------

### 1.3 資料背景與原始目的

* 資料背景：

本資料集為美國農業部針對中部草原地帶之農地所做的土壤侵蝕(Soil Erosion)和土壤有機質(Soil Organic Carbon)的觀察型資料。

* 原始目的：

本資料蒐集之目的為依照土壤侵蝕的程度來規劃美國農業部執行土壤環境維護之資源應用及分配。此資料即包含雨蝕、風蝕程度以及各種農地的基本資料、使用紀錄。

-------------------------------------------------

### 1.4 資料變數

* 原始資料：

本報告的原始資料共有37個變數，其中soil erosion 此變數本為一個連續型變數，我們根據網路上土壤報告，將其數值大於5設為severe，小於5設為minor，作為預測二元變數。其餘為農地、土壤本身性質、農作收穫量等等的解釋變數。

* 變數說明：

原始資料扣除ID、名稱、重複及有明顯共線性(soil erosion = watereros + winderos)之變數以後，具有15個類別變數、16個連續變數，共有31個實質變數，以下為各變數的說明。

```{r variable names, echo = F}

soil.var <- read.csv("soil_vars.csv")

soil.var %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped","bordered"))
  
```

-------------------------------------------------

### 1.4 研究問題

Prediction:

希望藉由本資料去預測Soil Erosion(土壤侵蝕程度)變數。 

-------------------------------------------------

## 2. 資料清理

### 2.1 去除不必要的變數

對於資料集而言沒有作用的變數以及存在嚴重關聯性問題的變數，都是我們不需要的。例如：`sci `、`scier `，這些變數都與`soil erosion`有關聯。此外，例如：`musym`、`crop1`...等對於資料集而言均沒有作用。我們便以上述來去除對於資料集而言不需要的變數，並且我們會在EDA部分再詳細探討關聯性問題。

### 2.2 遺失值處理

我們接下來計算各變數的遺失值比例，若遺失值比例大於百分之八十即予去除該變數。下圖中的左圖為**Proportion Plot of Missing Data**，橫軸為變數名稱，縱軸為遺失值比例。若變數的遺失值比例超過百分之八十則予以刪除。由左圖可以看出只有一個變數`awc_r`存在遺失值比例(比例為0.003)，我們便對`awc_r`補值。

```{r, echo=FALSE,results='hide',fig.keep='all', fig.align='center',out.extra='angle=90'}
aggplot <- aggr(missing, numbers = T, prop = T, sortVars = T
                       , col =  c('skyblue','orange'),labels = names(missing),
                       cex.axis = 0.7, gap = 3
                       ,ylabs = c("Histogram of missing data", "Patterns"))
```

-------------------------------------------------

我們使用mice套件內的sample方法補值。下圖中的左圖為**Proportion Plot of Missing Data**，橫軸為變數名稱，縱軸為遺失值比例，可以看出最終無遺失值存在。刪減以及補值後共有`22`個變數，並以刪減完成的變數去做探索性資料分析。

```{r, echo=FALSE,results='hide',fig.keep='all', fig.align='center',out.extra='angle=90'}
aggreplot <- VIM::aggr(soil.int, numbers = T, prop = T, sortVars = T,
                       labels = names(soil.int), cex.axis = 0.7, gap = 3,
                       ylabs = c("Histogram of missing data", "Patterns"))
```

-------------------------------------------------

## 3. 探索性資料分析

在執行EDA之前，我們先確認目前資料集的變數數量為`22`個變數。我們先將變數細分為類別型變數(categorical variables)以及連續型變數(continuous variables) ，其中有`11`個類別型變數以及`11`個連續型變數。

### 3.1 類別型變數

下圖為**Bar Plot of soil_erosion**，我們反應變數的長條圖，橫軸為`soil_erosion`，縱軸為`Count`。可以看出`soil_erosion(土壤侵蝕程度)`變數以`Minor`居多，代表土壤受到侵蝕程度大部分是輕微的。

```{r , echo = F}
ggplot(data = soil_agg) + geom_bar(fill = "goldenrod2", aes(x = factor(soil_erosion))) + 
    ggtitle("Bar Plot of soil_erosion")+
  ylab("Count") + xlab("soil_erosion(土壤侵蝕程度)")
```

-------------------------------------------------

下圖為**Bar Plot of tfact**，橫軸為`tfact`，縱軸為`Count`。可以看出`tfact(土壤侵蝕容忍程度)`變數的值主要集中在5，代表該資料大部分的土壤，受侵蝕容忍程度是高的，也與我們的反應變數`soil erosion`相互對照。

```{r , echo = F}
ggplot(data = soilcat) + geom_bar(fill = "tan2", aes(x = factor(tfact))) + 
    ggtitle("Bar Plot of tfact")+
  ylab("Count") + xlab("tfact(土壤侵蝕容忍程度)")
```

-------------------------------------------------

### 3.2 連續型變數
下圖為**Density Plot of yield3**，橫軸為`yield1`, `yield2`, `yield3`，縱軸為`Density`。此圖為第一年到第三年收穫量的Density plot，可以看出許多土地並無種植作物，因此此類土地汙染也較不嚴重。

```{r , echo = F}
p4 <- ggplot(soilcon, aes(x=yield1)) + geom_density(fill=2) + xlab(paste0((colnames(dt)[col]), 'yield1' ,'\n', 'Skewness: ', round(skewness(soilcon$yield1, na.rm = TRUE), 2))) + theme( legend.key.width=unit(10,"inches"))
p5 <- ggplot(soilcon, aes(x=yield2)) + geom_density(fill=3) + xlab(paste0((colnames(dt)[col]), 'yield2' ,'\n', 'Skewness: ', round(skewness(soilcon$yield2, na.rm = TRUE), 2))) + theme( legend.key.width=unit(10,"inches"))
p6 <- ggplot(soilcon, aes(x=yield3)) +  geom_density(fill=4) + xlab(paste0((colnames(dt)[col]), 'yield3' ,'\n', 'Skewness: ', round(skewness(soilcon$yield3, na.rm = TRUE), 2))) + theme( legend.key.width=unit(10,"inches"))
gridExtra::grid.arrange(p4, p5, p6, ncol=2)
```

下圖為**Correlation Plot 1**，橫軸為變數名稱，色條為相關係數，色條顏色越接近藍色則正關聯性越高，越接近紅色則負關聯性越高。由下圖可得知存在不同變數關聯性過高問題。我們便去除對於資料集而言不需要的變數。

```{r , echo = F}
corr.soil <- cor(soilcon2)
corrplot(corr.soil, method = "color", diag = F, type = "upper",
         tl.cex = 0.5 )
```

下圖為**Correlation Plot 2**，是我們刪除一些較關聯性過高的變數後所繪製的。橫軸為變數名稱，色條為相關係數，色條顏色越接近藍色則正關聯性越高，越接近紅色則負關聯性越高，可看出已無關聯性過高的問題。

```{r , echo = F}
soilcon2 <- soilcon2 %>%
  dplyr::select(-c(sci, scier))
corr.soil <- cor(soilcon2)
corrplot(corr.soil, method = "color", diag = F, type = "upper",
         tl.cex = 0.5 )
```

-------------------------------------------------

## 4. 預測分析

資料切割的方式，我們依照 80 % training 以及 20 % testing 的方式進行切割。其中，training data中的30 % 會做為CV的validation資料，在執行模型CV的時候會進行設定。為了符合5次Bootstrap cross-testing的標準，我們再將testing data執行5次的`createDataPartition()`，隨機抽樣的機率設定為 p = 0.8。在切割完成以後再個別將結果設定為test_1 ~ test_5，以利建模後的cross-testing執行。 

### 4.1 logistic regression without any variable/model selection
使用`Caret`套件`train`函數，並帶入10 fold的cross validation去建立模型。下圖為預測與實際值畫出的ROC圖。

```{r, echo=FALSE}
xgb.probs <- predict(model1,soil_test,type="prob")
xgb.ROC <- roc(predictor=xgb.probs$Severe   ,
               response=soil_test$soil_erosion,
               levels=rev(levels(soil_test$soil_erosion)))
plot(xgb.ROC,main="Soil Erosion ROC \n Area under the curve: 0.9674")
```

我們接著將test1到test5代入入模型中，分別得出不同的Accuracy，於下表呈現，最終得知平均Accuracy為91.4%。
```{r, echo=FALSE}
Accuracy <- c("0.913", "0.914", "0.914","0.913" , "0.914", "0.914")
data <- t(data.frame(Accuracy))
colnames(data) <- c("test1","test2","test3","test4","test5", "test_average")
kable(data) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "center") %>%
  column_spec(7, bold = T, color = "grey", background = "skyblue")
```
-------------------------------------------------

### 4.2 logistic regression with forward selection
在執行向前逐步迴歸建模前，我們先分別建立包含全部X變數的`Full model`和只包含截距項的`Null model`，之後再帶入我們的向前逐步迴歸去建立模型。下圖為Test data預測值與實際值的ROC圖。

```{r, echo=FALSE}
test_prob <- predict(model2, newdata = soil_test, type = "response")
test_roc <- roc(soil_test$soil_erosion ~ test_prob, plot = TRUE, print.auc = TRUE)
```



我們接著將test1到test5代入入模型中，分別得出不同的Accuracy，於下表呈現，最終得知平均Accuracy為91.2%。
```{r, echo=FALSE}
Accuracy <- c("0.911", "0.911", "0.912","0.912" , "0.911", "0.912")
data <- t(data.frame(Accuracy))
colnames(data) <- c("test1","test2","test3","test4","test5", "test_average")
kable(data) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "center") %>%
  column_spec(7, bold = T, color = "grey", background = "skyblue")
```
-------------------------------------------------

### 4.3 logistic lasso regression
將lambda值設為從0.0001到0.01去做建立模型。下圖為預測與實際值畫出的ROC圖。

```{r, echo=FALSE}
xgb.probs <- predict(model3,soil_test,type="prob")
xgb.ROC <- roc(predictor=xgb.probs$Severe   ,
               response=soil_test$soil_erosion,
               levels=rev(levels(soil_test$soil_erosion)))
plot(xgb.ROC,main="Soil Erosion ROC \n Area under the curve: 0.9059")
```


我們接著將test1到test5代入入模型中，分別得出不同的Accuracy，於下表呈現，最終得知平均Accuracy為82.9%。
```{r, echo=FALSE}
Accuracy <- c("0.827", "0.832", "0.829","0.831" , "0.828", "0.829")
data <- t(data.frame(Accuracy))
colnames(data) <- c("test1","test2","test3","test4","test5", "test_average")
kable(data) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "center") %>%
  column_spec(7, bold = T, color = "grey", background = "skyblue")
```
-------------------------------------------------

### 4.4 logistic rigid regression
將lambda值設為從0.0001到0.01去做建立模型。下圖為預測與實際值畫出的ROC圖。

```{r, echo=FALSE}
xgb.probs1 <- predict(model4,soil_test,type="prob")
xgb.ROC <- roc(predictor=xgb.probs1$Severe   ,
               response=soil_test$soil_erosion,
               levels=rev(levels(soil_test$soil_erosion)))
plot(xgb.ROC,main="Soil Erosion ROC \n Area under the curve: 0.8963")
```


我們接著將test1到test5代入入模型中，分別得出不同的Accuracy，於下表呈現，最終得知平均Accuracy為82%。
```{r, echo=FALSE}
Accuracy <- c("0.818", "0.823", "0.820","0.821" , "0.820", "0.820")
data <- t(data.frame(Accuracy))
colnames(data) <- c("test1","test2","test3","test4","test5", "test_average")
kable(data) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "center") %>%
  column_spec(7, bold = T, color = "grey", background = "skyblue")
```
-------------------------------------------------

### 4.5 Logistic GAM 模型

```{r read GAM model fit, echo = F}

gam.fit3 <- readRDS("RDS files/gam_fit.rds")

soil_train <- readRDS("RDS files/soil_train.rds")

  # testing data
test_1 <- read_rds("RDS files/test_1.rds")
test_2 <- read_rds("RDS files/test_2.rds")
test_3 <- read_rds("RDS files/test_3.rds")
test_4 <- read_rds("RDS files/test_4.rds")
test_5 <- read_rds("RDS files/test_5.rds")

  ## convert soil erosion to "Severe" to 1, "Minor" to 0
soil_train$soil_erosion <- ifelse(soil_train$soil_erosion == "Severe", 1, 0)
soil_train$soil_erosion <- factor(soil_train$soil_erosion, levels = c(0, 1))

  # Convert testing datas into proper factors
#test_1 convert
test_1$soil_erosion <- ifelse(test_1$soil_erosion == "Severe", 1, 0)
test_1$soil_erosion <- factor(test_1$soil_erosion, levels = c(0, 1))

#test_2 convert
test_2$soil_erosion <- ifelse(test_2$soil_erosion == "Severe", 1, 0)
test_2$soil_erosion <- factor(test_2$soil_erosion, levels = c(0, 1))

#test_3 convert
test_3$soil_erosion <- ifelse(test_3$soil_erosion == "Severe", 1, 0)
test_3$soil_erosion <- factor(test_3$soil_erosion, levels = c(0, 1))

#test_4 convert
test_4$soil_erosion <- ifelse(test_4$soil_erosion == "Severe", 1, 0)
test_4$soil_erosion <- factor(test_4$soil_erosion, levels = c(0, 1))

#test_5 convert
test_5$soil_erosion <- ifelse(test_5$soil_erosion == "Severe", 1, 0)
test_5$soil_erosion <- factor(test_5$soil_erosion, levels = c(0, 1))
```

在進行`Logistic GAM`建模以前，我們必須先將training data 以及 testing data的Y變數的二元變數轉換為0跟1個Factor。轉換完成以後，我們再開始進行建模。因為Logistic GAM為Binary的Y變數，因此沒有辦法透過簡易繪圖的方式抓出應該轉為非線性關聯的X變數。因此我們先將沒有刪減變數也沒有設定spline的full model進行Logistic GAM，並利用`summary(gam.fit)`的變數顯著性檢定將不顯著的連續型變數進行Spline的設定。我們針對需要調整的兩個變數: `muacres`、`yield3`進行多次的DF設定，並利用`ANOVA(model1, model2,test = 'Chisq')`指令執行模型的選擇。我們最後選出的模型是yield3分3段的spline, `muacres`分10段的spline的Logistic GAM模型。  
\  

確立我們的Logistic GAM 模型以後，我們利用`plot.GAM()`針對我們有興趣的幾個變數進行簡易的繪圖，以利了解造成嚴重土壤侵蝕的潛藏原因。  
\  

```{r GAM yield3 tillage, fig.align='center', fig.show='hold', fig.cap=" 左圖: 第3年收穫量   ,  右圖: 灌溉系統差異", echo=F, fig.width= 4,fig.height= 4}
library(gam)
gam.fit3 <- readRDS("RDS files/gam_fit.rds")
plot.Gam(gam.fit3, terms = "s(yield3, df = 3)", se = T, col = "orange", lwd = 3)
  # we can observe that 0 has the least log(P(x) / 1 - P(x)) =  -1, causes severe erosion
    # If yield > 0 then log(P(x) / 1 - P(x)) is around 0.5 but slightly increases as yield3 goes up 
      # planting reduces erosion, while naked ground suffers extreme erosion

plot.Gam(gam.fit3, terms = "tillage", se = T, col = "orange", lwd = 3)
  # Tillage system reduces soil erosion, water in soil can mitigate erosion
```  
\  

由第3年收穫量與Logistic GAM的圖可知，若當年沒有種植農作物則土壤侵蝕嚴重的機率將大幅上升。反之，若當年有種植作物則較不容易有嚴重的土壤侵蝕，但農作的多寡卻不太影響嚴重土壤侵蝕發生的機率，或許農作收穫量受到農地大小和農作種植面積的影響。  
\  

灌溉系統和土壤侵蝕嚴重的機率相關圖則顯示，有灌溉系統的農地較不容易發生嚴重的圖壤侵蝕，且此效果十分顯著。由此可知，水分應能降低土壤侵蝕的發生機率。  
\  

接著我們將5組Bootstrap得出的testing data分別進行預測，並個別計算出其預測的準確率。  

```{r GAM 5 predictions, echo = F, warning=F, results='hide'}
#### Training data 
set.seed(7783)
gamPred_0 <- predict.Gam(gam.fit3, type = "response")

## Accuracy
gamPredNum_0 <- ifelse(gamPred_0 >= 0.5, 1, 0)

trainGamPred <- factor(gamPredNum_0, levels = c(0,1))
trainGamAct <- soil_train$soil_erosion

g0 <- mean(trainGamPred == trainGamAct) # 0.9153355 (91.53 %)

#### test_1
set.seed(1333)
gamPred_1 <- predict.Gam(gam.fit3, newdata = test_1, type = "response")

### test_1 GAM Prediction (Accuracy)
gamPredNum_1 <- ifelse(gamPred_1 >= 0.5, 1, 0)

test_1gampred <- factor(gamPredNum_1, levels = c(0,1))
test_1gamact <- test_1$soil_erosion

g1 <- mean(test_1gampred == test_1gamact) # 0.9123926 (91.24 %)

#### test_2
set.seed(1365)
gamPred_2 <- predict.Gam(gam.fit3, newdata = test_2, type = "response")


  ### test_2 GAM Prediction (Accuracy)
gamPredNum_2 <- ifelse(gamPred_2 >= 0.5, 1, 0)

test_2gampred <- factor(gamPredNum_2, levels = c(0,1))
test_2gamact <- test_2$soil_erosion

g2 <- mean(test_2gampred == test_2gamact) # 0.9139236 (91.39 %)

#### test_3
set.seed(1345)
gamPred_3 <- predict.Gam(gam.fit3, newdata = test_3, type = "response")


### test_3GAM Prediction (Accuracy)
gamPredNum_3 <- ifelse(gamPred_3 >= 0.5, 1, 0)

test_3gampred <- factor(gamPredNum_3, levels = c(0,1))
test_3gamact <- test_3$soil_erosion

g3 <- mean(test_3gampred == test_3gamact) # 0.9138386 (91.38%)

#### test_4
set.seed(1873)
gamPred_4 <- predict.Gam(gam.fit3, newdata = test_4, type = "response")


### test_4 GAM Prediction (Accuracy)
gamPredNum_4 <- ifelse(gamPred_4 >= 0.5, 1, 0)

test_4gampred <- factor(gamPredNum_4, levels = c(0,1))
test_4gamact <- test_4$soil_erosion

g4 <- mean(test_4gampred == test_4gamact) # 0.9137535 (91.38%)

#### test_5
set.seed(1952)
gamPred_5 <- predict.Gam(gam.fit3, newdata = test_5, type = "response")


### test_5 GAM Prediction (Accuracy)
gamPredNum_5 <- ifelse(gamPred_5 >= 0.5, 1, 0)

test_5gampred <- factor(gamPredNum_5, levels = c(0,1))
test_5gamact <- test_5$soil_erosion

g5 <- mean(test_5gampred == test_5gamact) # 0.9135834 (91.36%)

### The Average GAM Accuracy

gamAVG <- (sum(g1,g2,g3,g4,g5)/5)

```  
\  

---

```{r GAM ACC, echo=FALSE}
Accuracy <- c("0.9124", "0.9139", "0.9138","0.9137" , "0.9135", "0.9135")
data <- t(data.frame(Accuracy))
colnames(data) <- c("test1","test2","test3","test4","test5", "test_average")
kable(data) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "center") %>%
  column_spec(7, bold = T, color = "grey", background = "skyblue")
```  
\  

五次Testing Data預測的平均準確率即為91.35%，預測的準確率十分高。  
我們推測準確率這麼高的原因在於Logistic GAM的模型建構缺少交叉驗證的步驟，因此在做testing data的預測時應該多做幾組bootstrap testing data的預測，再來計算平均的預測準確率。  
\  

---

### 4.6 SVM 模型  
\  



---

### 4.7 類神經網絡  
\  

在執行類神經網絡的建模以前，我們必須先對原本的training data做轉換才能做模型建模。首先，我們必須將我們的Y變數(soil_erosion) 拆解為兩個Dummy variable。接著，我們再將其他類別型的X變數也拆解為Dummy variables，並移除原始的類別型變數。完成上述的步驟以後，再寫出完整的模型預測方程式。  
\  

完成準備步驟以後，我們先用`caret`套件協助判定類神經網絡模型的參數設定。如下圖所示，第一隱藏層的取1個節點具有最低的RMSE，第二隱藏層的節點則較不明顯。我們以表格的方式招出最好的組合為: 第1隱藏層1個節點、第2隱藏層1個節點。  
\  

```{r neural tune plot, fig.align='center', fig.width=6, fig.height=4, fig.cap="Caret Tune model 結果呈現", echo = F}

nnt.tune <- read_rds("RDS files/nnt_tune_real.rds")

plot(nnt.tune)# no obvious results

nnt.tune$results %>% kable() %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "center") %>%
  row_spec(2, bold = T, color = "grey", background = "skyblue")
```  
\  

決定好neural network有幾個隱藏層和幾個節點以後，我們將再對training data進行一次類神經網絡的模型建構，得到最終的neural network模型。最終的模型如下圖所示。之後我們再對5組testing data進行預測，並計算出平均的Accuracy。  
\  

```{r nntmodel plot, echo = F, fig.width=8, fig.height=6, fig.align='center', fig.cap = "Neural Network 結構"}

knitr::include_graphics("RDS files/neuralnetPlot.jpeg")
```  
\  

在做預測之前，我們必須先將testing data轉成和training data相同的格式。除此之外，neural network的建模過程會將判斷為沒有效果的變數自動刪除，且neural network的預測指令亦不接受testing data保留這些被刪除的變數。因此，我們必須再將testing data的變數做調整。經過繁雜的預測過程以後，我們得到5組的Accuracy 以及平均的Accuracy如下表:  
\  

---

```{r nn Acc, echo = F}
Accuracy <- c("0.6965", "0.6965", "0.6965", "0.6965", "0.6965", "0.6965")
data <- t(data.frame(Accuracy))
colnames(data) <- c("test1","test2","test3","test4","test5", "test_average")
kable(data) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "center") %>%
  column_spec(7, bold = T, color = "grey", background = "skyblue")
```  
\  

上表5組testing data的 Accuracy皆為0.6965，不太符合理想的狀況。我們推測是因為稍早tune出的模型參數過於簡單，此neural network 僅有2個隱藏層，且各層只有1個節點。實際觀察confusion matrix更可發現預測的結果是將所有觀察值歸類為`Minor`。因此5組testing data的預測準確率也完全相同。再深入檢視neural network回傳的預測分群機率可發現，實際為"Minor"的資料分為"Minor"的機率為0.91左右，但實際為"Severe"卻分為"Severe"的機率只有0.46而已。預測機率超過0.5的會被分類為此結果，因此所有的資料筆數最後都被分為"Minor"。也就造成5組testing data預測結果都為"Minor"，Accuracy也都一樣的詭異狀況。  
\  

```{r prednn show, echo=F}

prednn_show <- read_rds("RDS files/prednn.rds")

head(prednn_show) %>% kable() %>%
    kable_styling(bootstrap_options = "striped", full_width = F, position = "center") %>%
  row_spec(4, bold = T, color = "grey", background = "skyblue")
```  
\  

&nbsp;  


-------------------------------------------------

### 4.8 KNN

這筆資料集的解釋變數包含`10`個類別變數，而其中有不少Nominal variables，然而這些變數無法計算歐幾里得距離矩陣，因此我們先移除這些變數來建立KNN模型。

下圖為**Error Rate of Different K**，在Testing data中，使用不同的`k`的所跑出來的Error rate會不同，我們將挑選Error rate最低的k作為參數來建模。

```{r, echo=FALSE}
k_plot <- qplot(1:30, error_rate_k*100, 
                xlab = "K",
                ylab = "Error Rate %",
                ylim = c(0,30), 
                geom=c("point", "line"),colour="red")
k_plot
```

-------------------------------------------------

而我們將test1到test5所使用的不同`k`參數，以及代入模型後得出的不同Accuracy於下表呈現，最終得知平均Accuracy為76.6%。

```{r, echo=FALSE}
k <- c("9", "9", "13", "9", "7", " ")
Accuracy <- c("0.765", "0.766", "0.764","0.767" , "0.767","0.766")
data <- t(data.frame(k, Accuracy))
colnames(data) <- c("test1","test2","test3","test4","test5","test_average")
kable(data) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "center") %>%
  column_spec(7, bold = T, color = "grey", background = "skyblue")
```
-------------------------------------------------

### 4.9 LDA

LDA的假設希望變數為常態，然而這筆資料集的變數並不是常態分佈，因此可能較不適合用LDA來建模。下圖為**Plot of LDA Model**，可以看出分類狀況。

```{r, echo=FALSE}
plot(lda, col="brown1")
```

-------------------------------------------------

我們接著將test1到test5代入入模型中，分別得出不同的Accuracy，於下表呈現，最終得知平均Accuracy為86.1%。

```{r, echo=FALSE}
Accuracy <- c("0.86", "0.862", "0.862","0.862" , "0.861", "0.861")
data <- t(data.frame(Accuracy))
colnames(data) <- c("test1","test2","test3","test4","test5", "test_average")
kable(data) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "center") %>%
  column_spec(7, bold = T, color = "grey", background = "skyblue")
```

-------------------------------------------------

### 4.10 Boosting

在Boosting建模前，我們首先將類別變數分別都轉成Dummy Variables，並設定`eta = 0.01`, `colsample_bytree = 0.95`, `subsample = 0.55`, `max_depth = 1`, `alpha = 0.1`，然後找出最佳的`nrounds`參數。下圖為**Avg.Performance in CV**，我們希望能找出Train Error與Test Error最小差距時的`nrounds`，並得出`nrounds = 4959
`
```{r, echo=FALSE}
plot(x=1:nrow(tmp), y= tmp$train_error_mean, col='orange', xlab="nround", ylab="Error rate", main="Avg.Performance in CV",abline(v=4959,col="red",lty=2))
points(x=1:nrow(tmp), y= tmp$test_error_mean, col='skyblue') 
legend("topright", pch=1, col = c("orange", "skyblue"), 
       legend = c("Train", "Test") )
```

-------------------------------------------------

我們接著將test1到test5代入入模型中，分別得出不同的Accuracy，於下表呈現，最終得知平均Accuracy為92.1%。

```{r, echo=FALSE}
Accuracy <- c("0.919", "0.923", "0.921","0.921" , "0.921", "0.921")
data <- t(data.frame(Accuracy))
colnames(data) <- c("test1","test2","test3","test4","test5", "test_average")
kable(data) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "center") %>%
  column_spec(7, bold = T, color = "grey", background = "skyblue")
```

-------------------------------------------------

### 4.11 Decision Tree

決策樹是將資料進行一層一層的分割，而分割的原則是要得到最大的資訊增益。資訊量通常是以`熵(Entropy)`以及 `Gini不純度(Gini Impurity)`為衡量標準。

下圖為**決策樹模型**，所使用到的變數有`yield3(第三年收穫量)` `sciom(生物分解指標)`  `tillage(灌溉方式)`，可以看出Minor分類的情形比Severe好。

```{r, echo=FALSE}
tree_plot <- fancyRpartPlot(tree_model$finalModel)
```

-------------------------------------------------

我們接著將test1到test5代入進模型中，分別得出不同的Accuracy，於下表呈現，最終得知平均Accuracy為81.7%。

```{r, echo=FALSE}
Accuracy <- c("0.815", "0.818", "0.818","0.816","0.818", "0.817")
data <- t(data.frame(Accuracy))
colnames(data) <- c("test1","test2","test3","test4","test5", "test_average")
kable(data) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "center") %>%
  column_spec(7, bold = T, color = "grey", background = "skyblue")
```

-------------------------------------------------

### 4.12 Bagging

`Bagging` 是 `Bootstrap Aggregating` 的簡稱，透過統計學的 `Bootstrap sampling` 得到不同的訓練資料，然後根據這些訓練資料得到一系列的預測結果，並加以整合與平均，可以有效降低單一模型的變異程度。另外
，一般在做 train 時會把手上的 label data 切成 training set 跟  validation set，但使用 bagging 的時候，不用如此，同樣可以擁有 validation 的效果，叫做 `Out-of-bag validation`。

-------------------------------------------------

最後以 `bootstrap replications = 27` ，所得到的 `Out-of-bag estimate of misclassification error`最小，因此將模型參數`nbagg`設為27，再將test1到test5代入進模型中，分別得出不同的Accuracy，於下表呈現，最終得知平均Accuracy為90.8%。

```{r, echo=FALSE}
Accuracy <- c("0.908", "0.908", "0.909","0.910","0.907", "0.908")
data <- t(data.frame(Accuracy))
colnames(data) <- c("test1","test2","test3","test4","test5", "test_average")
kable(data) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "center") %>%
  column_spec(7, bold = T, color = "grey", background = "skyblue")
```

-------------------------------------------------

### 4.13 Random Forest

隨機森林是結合多棵決策樹，並加入隨機分配的訓練資料，以大幅增進最終的預測結果，然而決策樹的數量是透過下圖 `最小錯誤率`來進行選擇，並可以由`Minor` `Servere` `OOB` 三條線，看出在決策樹數量皆為30時，所得到的錯誤率是最低的，因此最後選擇`tree=30`。

```{r, echo=FALSE}
plot(randomforest_model, lwd=2)
legend("topright", colnames(randomforest_model$err.rate),
          lty=1:4, lwd=2, col=1:4)
abline(v=30, col="blue")
```

-------------------------------------------------

於是，將模型參數`ntree`設為30，接著將test1到test5代入進模型中，分別得出不同的Accuracy，於下表呈現，最終得知平均Accuracy為96.5%。

```{r, echo=FALSE}
Accuracy <- c("0.966", "0.965", "0.966","0.966","0.966", "0.965")
data <- t(data.frame(Accuracy))
colnames(data) <- c("test1","test2","test3","test4","test5", "test_average")
kable(data) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "center") %>%
  column_spec(7, bold = T, color = "grey", background = "skyblue")
```

-------------------------------------------------

## 5. 結論

做一個accuracy table~